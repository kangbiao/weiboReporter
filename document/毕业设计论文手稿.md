
2.1.1基于整个万维网的信息抓取这种方法是从一些基础URL出发，采用深度优先或者广度优先策略进行爬取，逐步扩展到整个万维网。这种方法的优点是信息全、覆盖面广，主要被各大搜索引擎所使用，其缺点是爬行耗时长，不利于最新信息的搜集，此外由于所需的硬件资源多，只有少数几个服务商能够采用这种技术。
2.1.2基于主题的信息爬取这种爬虫的特点是先进行网络内容的判断，如果与想要爬取的主题一致，再进行爬取。由于只对某一个主题进行爬取，所以此种爬虫的优点是采集的页面针对性很强，效率也较高，比较适用于专门对某一个问题进行研宄的研究人员。如果想要先采集数据，再进行数据分析，则此种爬虫不适用。
2.1.3增量式信息爬取此种爬虫是针对第一种网络的对新信息反应慢的缺点而提出的。它只对有变化的网页进行再爬取，而不对没有变化 网页进行爬取。此种爬虫的效率高，在信息量大的今天有很强的实际意义，它的难点在于如何判断网页是否有变化，所以它的爬行效果完全取决于所使用的判断算法的有效性。
分布式网络爬虫。由于目前网络上的信息量巨大，如果只使用一台计算机进行采集，耗时巨大，信息更新也慢，所以研宄人员提出了使用很多台计算机同时对网络进行信息采集，这样一来可以大大提高爬行的速度和效率，但是如何进行爬行任务的分工以及多台计算机如何协调配合，使每台的工作压力相对均衡是此种爬虫的设计难点。

2.3.1微博短文本挖掘。由于微博的长度较短，所以可以把微博上的内容看成是不同短文本的集合，这个集合有一些显而易见的特点：文本口语化，存在大量错别字、谐音字、符号等;信息碎片化，文本的特征不明显，意思表达不清楚;信息量巨大，又由于转发量大，原创数量小，微博内容的重复性很高。为弥补短文本特征值的不足，S.Bharath将用户所发的微博按所涉领域进行划分，然后对不同微博进行分类，这样提高了短文本的特征值。而针对微博内容的重复性，王永恒提出了用于聚合大量短文本的方法，通过聚合，可以使得重复性的文本大大减少，提高了剩余文本间的区分度
2.3.2情感倾向性分析。人们在所发的微博中，通常都带有自己的情感倾向。这些情感倾向通过使用语气词、具有判断性的词汇表达出来，情感倾向对于研究诸如用户满意度、大众对社会热点的态度等尤为重要。分析人们的情感倾向，最常用的方法就是建立情感词库，然后进行词汇的匹配和表情、语气等的标记，以此来发现人们的倾向另外，也出现了一些情感分类的算法，如最大信息熵、向量机、贝叶斯算法等，在使用此类算法时，为了提高正确率，除了进行情感的分类外，还需要结合上下文进行判断。
2.3.3话题趋势检测。由于微博具有快速更新的特点，一个热点话题可以在极短的时间内广泛传播，造成巨大的影响。所以快速的找出热门话题，判断话题传播的方向、发展的趋势等，就能更好的掌握舆论的走势，为下一步的行动争取时间。最常见的话题趋势检测就是进行词频的检测，通过检测某个词出现的频率，就能判断某个话题的热门程度。除此之外，可以结合PageRank算法，将转发量、作者的重要性、词频等作为加权因素，就可以计算出此话题的重要性，并判断其未来的走势。

传播学大师麦克卢汉认为：“媒介是社会发展的基本动力，也是区分不同社会形态的标志，每一种新媒介的产生与运用，宣告我们进入了一个新时代。”以微博的发展情况来看，相比于社交网站，它越来越像一个新闻媒体，而随着微博发展壮大，旧有媒介格局下的公众舆论环境受到根本性冲击。2009年，微博客受到了人们的极大关注。简单快捷的操作方式、随时随地发布信息的互动形式，成为互联网的一个亮点。麦克卢汉认为：“任何媒介对个人和社会的影响都是由于新的尺度产生的，我们的任何一种延伸都要在我们的事务中引入一种新的尺度。”2009年7月29日，Twitter把首页那句“你在做什么?”改做“分享和发现世界各处正在发生的事。”这次改变象征着推特已经不仅仅满足于做一个人际沟通工具，推特的运营者们发现了微博这个平台的强大力量，作为一种传播媒介，它信息源丰富、传播速度快、扩张力度强、影响力大，正在带来一种新的媒体格局。全球语言监测机构(Global Language Monitor)公布的数据显示，微博客“Twitter”成为2009年最热门的英语单词。
新浪微博作为国内最有影响力的微博，截止2012年初已经有2亿注册用户，已经成为公认的重要舆论阵地。微博以其独特的传播模式，成为近两年来舆论场上一支最活跃、开放，且屡立奇功的生力军[12]。2009年末上海地铁站[13]出现事故以后，不少在现场的网友都在第一时间内把图片发布到微博上，让新闻事件中传播的主体发生了变化。微博有最快的信息传输工具——手机，最庞大的通讯员队伍——新闻事件的亲历者、目击者，以及最广泛的传播员——网民，因而逐渐成为“最快捷最草根的新闻发布厅” [14]。网络技术[15]在微博的实现过程中起着至关重要的作用。它把互联网上分散的资源融为有机整体，实现资源的全面共享和有机协作，使人们能够透明地使用资源的整体能力并按需获取信息。资源包括高性能计算机、存储资源、数据资源、信息资源、知识资源、专家资源、大型数据库、网络、传感器等。当前的互联网只限于信息共享，网络则被认为是互联网发展的第三阶段。网络可以构造地区性的网络、企事业内部网络、局域网网络，甚至家庭网络和个人网络。网络的根本特征并不一定是它的规模，而是资源共享，消除资源孤岛。有了网络技术的基础，微博才可以蓬勃发展。随着互联网技术的发展和收集数据手段的提高，微博积累的数据越来越大，形成了大数据的问题。现在常用的分析手段，如概率统计等，已经很难满足分析的需要。很多时候，人们并不知道如何来分析这些数据，因为人们看不到数据，形成了 “数据盲人”。现在，可视化正作为一个新的领域蓬勃发展起来。可视化是解决这一问题的良好方法。可视化[16]可以粗略地被定义为通过图形的表现形式，进行信息传递、表达的过程。虽然我们今天谈到可视化一般是指利用计算机图形学和图像处理分析技术，将各种数据依据其特点转换为相应的图形图像，并提供界面实现人机交互的工作[17]，但早在计算机发明之前，可视化就已为人类广泛应用。化技术最早运用于计算机科学中，并形成了可视化技术的一个重要分支——科学计算可视化(Visualization in Scientific Computing)。科学计算可视化能够把科学数据，包括测量获得的数值、图像或是计算中涉及、产生的数字信息变为直观的、以图形图像信息表示的、随时间和空间变化的物理现象或物理量呈现在研究者面前，使他们能够观察、模拟等。可视化的三个方向——科学可视化、信息可视化、可视分析密切相关，同时又各有特点，有其研究内涵与外延[18]。科学可视化[19]处理的对象包括医学、气象环境学、化学工程、生命科学、考古学、机械等领域的具有空间几何特征数据的时空现象，对测量、实验、模拟等获得的数据进行绘制，并提供交互分析手段。研究重点包括对表面、体 绘制和对复杂数据中信息的选取、表达等。信息可视化[2G]是一门研究非空间数据的视觉呈现方法和技术的学科，通过提供非空间复杂数据的视觉呈现，帮助人们理解大量数据中蕴含的信息。信息可视化作为一个跨学科研究领域，综合地使用计算机图形学、视觉设计、人机交互、心理学等学科中的技术和理论，也与统计学、数据挖掘[21]、机器学习等有着相辅相成之处。可视分析[22]作为信息可视化与科学可视化领域最新发展的产物，主要包含四部分核心内容：分析推理技术，视觉呈现和交互技术，数据表示和转换技术和支持产生、表达和传播分析结果的技术。可视化在微博上的可视化研究已经 始。北京大学袁晓如教授带领其团队实现了新浪微博的可视化系统。其中包括数据的建模，分析和可视化。在这我们实现此系统时是个很好的参考。本文主要研究的问题如下.?1.研究微博传播分析及其可视化展示。2.研究微博转发量分析和微博评价分析传播。3.研究微博质量的评价和微博转发量覆盖面的表达式，画出可视化模型图。4.	一家公司想把自己的产品，通过一个100粉丝的账号，在3天内，传播给最多的受众。设计出最优方案实现微博营销。通常情况下，为了更好的研究这些问题，我们做如下约定和假设。(此假设只在本文中有效)。1.假设微博粉丝的转发能力不会随离消息源距离的增大而递减，即消息传播多轮之后仍会遇到转发能力强的微博粉丝。2.假设所有用户看到同一条微博只转发一次，不会重复转发。即转发网络末端的用户与企业账户之间有且只有一条路径相连。3.假设用户在看到微博后，如果要转发会立即转发，忽略延迟时间。且用户转发后其粉丝能够马上看到转发的微博。4.假设企业账号的100个粉丝中，有m个影响力较大的用户(粉丝较多且粉丝增速快)和(100-m)个影响力较小的普通用户(粉丝较少且粉丝增速慢)，设影响力较大的用户平均粉丝基数为M个，粉丝平均增速为P个/天，普通用户平均粉丝基数为N个，粉丝平均增速为q个/天(M>N， p>q)。5.每天新增粉丝发生在用户转发前，即每天新增的粉丝都可以收到当天的微博。6.假设3天内新增的粉丝都是影响力较小的普通用户。7.假设用户只能看到自己所关注的人发布或转发的微博。



背景

微博一词译自英文单词“micro-blog”，是博客的微缩版或者变体。这一理念最早由博客先驱埃文，威廉姆斯提出并于 2006 年创办了世界上最早提供微博服务的网站Twitter。我国微博应用始于2007年。“无处不在的沟通”是它的理念，“What are you doing”是它的宣传口号。目前，网络时代已经来临，上网购物#查阅信息等已经代替了传统的报纸商店，随着微博的兴起，国内学术界对微博的研究日益增多，迄今为止已有许多学者从教育学，传播学，经济学等多种学科视角，运用定量分析或者定性分析以及定量与定性相结合的研究方法对微博进行分析。时隔两年，2009 年国内互联网商家掀起建设微博平台热潮。2009年10月，新浪微博正式向公众开放，中国网民的微博时代到来，2010 年被称为微博元年。根据中国互联网络信息中心（CNNIC）的第31次“中国互联网络发展状况统计报告”，截至2012年12月底，我国网民规模达到5.64 亿，其中微博用户规模为3.09亿，比2011年底增长了54.7％，同时手机微博用户规模达到2.02亿，占微博用户的 65.6％。在这样的背景之下，微博及其相关话题成为政府、商务机构和寻常民众的热门话题，也是信息科学和社会科学共同关心的焦点之一。



## 国内外研究现状
随着互联网技术的发展和收集数据手段的提高，微博积累的数据越来越大，形成了大数据的问题。现在常用的分析手段，如概率统计等，已经很难满足分析的需要。很多时候，人们并不知道如何来分析这些数据，因为人们看不到数据，形成了“数据盲人”。现在，可视化正作为一个新的领域蓬勃发展起来。可视化是解决这一问题的良好方法。可视化可以粗略地被定义为通过图形的表现形式，进行信息传递、表达的过程。虽然我们今天谈到可视化一般是指利用计算机图形学和图像处理分析技术，将各种数据依据其特点转换为相应的图形图像，并提供界面实现人机交互的工作，但早在计算机发明之前，可视化就已为人类广泛应用。化技术最早运用于计算机科学中，并形成了可视化技术的一个重要分支——科学计算可视化(Visualization in Scientific Computing)。科学计算可视化能够把科学数据，包括测量获得的数值、图像或是计算中涉及、产生的数字信息变为直观的、以图形图像信息表示的、随时间和空间变化的物理现象或物理量呈现在研究者面前，使他们能够观察、模拟等。可视化的三个方向——科学可视化、信息可视化、可视分析密切相关，同时又各有特点，有其研究内涵与外延。科学可视化处理的对象包括医学、气象环境学、化学工程、生命科学、考古学、机械等领域的具有空间几何特征数据的时空现象，对测量、实验、模拟等获得的数据进行绘制，并提供交互分析手段。研究重点包括对表面、体 绘制和对复杂数据中信息的选取、表达等。

首先介绍Twitter的概念：Twitter是一个在线社交网络服务，允许用户发送和阅读简短的在140个字符内的消息，而这条消息称为"推文(tweet)"。Twitter平台包含了大量的用户信息W及丰富的内容，为预测研巧提供了重要的数据基础，因此也就出现了利用微博信息进行各种问题的预测。Jurgens等人利用地理位置推理算法对Twitter用户的地理位置进行了预测并取得了较好的预测结果。Bollen等人针对股票预测问题，用每天包含正面情感词和负面情感词的推文数比值来计算情感时间序列，通过自组织模糊神经网络(Self-Organizing Fuzzy Neural Network，简称SOFNN)预测道琼斯工业平均指数(Dow Jones Industrial Average，简称DJIA)。Wolfram等人使用SVM模型，从推文文本直接提取了特征属性来进行模型训练，预测纳斯达克指数。Zhang等人分析了微博情感和DJIA变化走势之间的相关性。欧美的学者还利用Twitter数据对大选进行了预测，而且逐渐成为了研究热点，取得了较好的预测效果。



主要研究的问题
在计算机网络和通信等学科领域，微博的信息数据特征是近年来关注的主要问题，很多微博研究都基于网络信息数据测量分析展开，微博信息数据是指微博数据中心中存储的各类数据，主要包括：微博用户档案、微博用户关系、微博消息热点话题等，它是此类研究方法的基础。在这类研究中，大多以微博消息传播三大构件为研究对象，以微博消息传播和微博成员组织为主要研究内容，目的在于发现微博中的用户、消息传播、热点话题
用户关系网络等的规律。主要研究的问题如下。
基于微博用户的研究，主要研究用户的行为特征及用户的影响力。
基于微博用户关系的研究，主要研究用户关系网络的基本属性、关系网络生成和演进、微博人员关系挖掘、微博用户人际关系特点。
基于微博内容的研究，主要研究微博消息内容特点、消息活跃时间特点、微博热点话题特点。
基于微博消息传播的研究主要研究微博消息传播的特点，微博消息传播影响力。

微博信息数据分析的研究方法
基于信息数据分析的微博研究方法，以分析微博信息数据为基础，致力于发现微博中的各种规律和特点它可以分为两个阶段： 信息数据获取和信息数据分析。在信息获取阶段，主要任务是获取大量微博信息数据，主要采用三种方法： 基于微博第三方应用程序接口(Application Programing Interface， API)编程实现微博信息爬取，利用网络爬虫在微博网页上爬取关键字信息，利用网络数据采集设备直接获取微博服务网络传输数据。在数据分析阶段，主要任务是对微博信息数据进行特征提取和分析，挖掘出微博中的关键特征，采用的主要方法包括统计学数据分析方法、复杂网络分析方法、数据分类及挖掘方法等。微博信息数据分析方法基本流程如下图1.1所示。
基于微博信息数据分析的研究是微博研究中非常重要的一个方向，它是开展其他微博研究的基础，能够积累大量的微博特征，对微博的理论和实践研究都至关重要。


## 非功能需求
非功能性需求是指依一些条件判断系统运作情形或其特性，而不是针对系统特定行为的需求。包括安全性、可靠性、互操作性、健壮性、易使用性、可维护性、可移植性、可重用性、可扩充性。
本课题设计程序的非功能需求主要包括：
用户界面美观，具有针对性。所有页面应该支持主流的浏览器（IE9.0+，Chrome，Firefox等）下正常显示，而且不同浏览器看到的页面外观应该一致。
在操作上要易于使用。使用合理的存储方式来存储数据，保证数据的独立性，也保证了系统的可移植性。
数据存储量。使用合理的存储硬件，能够易于存储爬虫爬去的零散数据，确保不会因为硬件原因导致数据存储出错，进而影响数据分析的完成。
系统可靠性。在实现一定的吞吐量的基础上，需要满足系统运行期间不会出现严重错误导致异常退出，从而导致系统不可用、数据丢失等严重故障。
可扩展性。可扩展性主要指微博爬虫的配置应该足够灵活，且内部功能应该能够支持扩展和重用，如果有其他数据分析的需求，核心处理代码应该能够重用。另外一方面，系统在实现时应该考虑到日后若信息量较大，则系统可相应增加服务器实现扩展。


## 数据需求
数据需求分析是从对数据进行组织与存储的角度，从用户视图出发，分析与辨别应用领域所管理的各类数据项（Data Items）和数据结构，形成数据字典的主要内容。


初始置换
其功能是把输入的64位数据块按位重新组合,并把输出分为L0、R0两部分，每部分各长32位，其置换规则为将输入的第58位换到第一位，第50位换到第2位……依此类推,最后一位是原来的第7位。L0、R0则是换位输出后的两部分，L0是输出的左32位，R0是右32位，例：设置换前的输入值为D1D2D3……D64，则经过初始置换后的结果为:L0=D58D50……D8；R0=D57D49……D7。
逆置换
经过16次迭代运算后，得到L16、R16,将此作为输入，进行逆置换，逆置换正好是初始置换的逆运算，由此即得到密文输出。










